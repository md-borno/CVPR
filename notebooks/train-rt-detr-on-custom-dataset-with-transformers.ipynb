{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n",
        "\n",
        "# How to Train RT-DETR on Custom Dataset\n",
        "\n",
        "---\n",
        "\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2304.08069-b31b1b.svg)](https://arxiv.org/pdf/2304.08069.pdf)\n",
        "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/lyuwenyu/RT-DETR)\n",
        "\n",
        "RT-DETR, short for \"Real-Time DEtection TRansformer\", is a computer vision model developed by Peking University and Baidu. In their paper, \"DETRs Beat YOLOs on Real-time Object Detection\" the authors claim that RT-DETR can outperform YOLO models in object detection, both in terms of speed and accuracy. The model has been released under the Apache 2.0 license, making it a great option, especially for enterprise projects.\n",
        "\n",
        "![RT-DETR Figure.1](https://storage.googleapis.com/com-roboflow-marketing/notebooks/examples/rt-detr-figure-1.png)\n",
        "\n",
        "Recently, RT-DETR was added to the `transformers` library, significantly simplifying its fine-tuning process. In this tutorial, we will show you how to train RT-DETR on a custom dataset."
      ],
      "metadata": {
        "id": "rIt8oNbfoNfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "NVjciAv6pZWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure your API keys\n",
        "\n",
        "To fine-tune RT-DETR, you need to provide your HuggingFace Token and Roboflow API key. Follow these steps:\n",
        "\n",
        "- Open your [`HuggingFace Settings`](https://huggingface.co/settings) page. Click `Access Tokens` then `New Token` to generate new token.\n",
        "- Go to your [`Roboflow Settings`](https://app.roboflow.com/settings/api) page. Click `Copy`. This will place your private key in the clipboard.\n",
        "- In Colab, go to the left pane and click on `Secrets` (ðŸ”‘).\n",
        "    - Store HuggingFace Access Token under the name `HF_TOKEN`.\n",
        "    - Store Roboflow API Key under the name `ROBOFLOW_API_KEY`."
      ],
      "metadata": {
        "id": "QIv6f0UhpgIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select the runtime\n",
        "\n",
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `L4 GPU`, and then click `Save`."
      ],
      "metadata": {
        "id": "E7Lb-k_Op3SR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6JGcExaSn_Ag",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a0db146-1433-4d7e-bdd4-24717f59f45c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Aug  7 18:30:34 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** To make it easier for us to manage datasets, images and models we create a `HOME` constant."
      ],
      "metadata": {
        "id": "6hPkdrkqsGii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(\"HOME:\", HOME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ki3zbgWisGR-",
        "outputId": "74c07717-5a27-4eef-ed7a-6965dcdedc58"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HOME: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install dependencies"
      ],
      "metadata": {
        "id": "n0XnOrHTqiVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q git+https://github.com/roboflow/supervision.git\n",
        "!pip install -q accelerate\n",
        "!pip install -q roboflow\n",
        "!pip install -q torchmetrics\n",
        "!pip install -q \"albumentations>=1.4.5\""
      ],
      "metadata": {
        "id": "gX_B-soMqDCH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd58bfed-ce2c-4a0f-b45b-db17ebc291ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "o-BR37YWs6Vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import requests\n",
        "\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "import albumentations as A\n",
        "\n",
        "from PIL import Image\n",
        "from pprint import pprint\n",
        "from roboflow import Roboflow\n",
        "from dataclasses import dataclass, replace\n",
        "from google.colab import userdata\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoImageProcessor,\n",
        "    AutoModelForObjectDetection,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision"
      ],
      "metadata": {
        "id": "O5e-XgeZs8al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference with pre-trained RT-DETR model"
      ],
      "metadata": {
        "id": "VS3d-ENgsSAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load model\n",
        "\n",
        "CHECKPOINT = \"PekingU/rtdetr_r50vd_coco_o365\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = AutoModelForObjectDetection.from_pretrained(CHECKPOINT).to(DEVICE)\n",
        "processor = AutoImageProcessor.from_pretrained(CHECKPOINT)"
      ],
      "metadata": {
        "id": "ihyGXa3Aq00O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run inference\n",
        "\n",
        "URL = \"https://media.roboflow.com/notebooks/examples/dog.jpeg\"\n",
        "\n",
        "image = Image.open(requests.get(URL, stream=True).raw)\n",
        "inputs = processor(image, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "w, h = image.size\n",
        "results = processor.post_process_object_detection(\n",
        "    outputs, target_sizes=[(h, w)], threshold=0.3)"
      ],
      "metadata": {
        "id": "oqCBGcnUtwB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Display result with NMS\n",
        "\n",
        "detections = sv.Detections.from_transformers(results[0])\n",
        "labels = [\n",
        "    model.config.id2label[class_id]\n",
        "    for class_id\n",
        "    in detections.class_id\n",
        "]\n",
        "\n",
        "annotated_image = image.copy()\n",
        "annotated_image = sv.BoundingBoxAnnotator().annotate(annotated_image, detections)\n",
        "annotated_image = sv.LabelAnnotator().annotate(annotated_image, detections, labels=labels)\n",
        "annotated_image.thumbnail((600, 600))\n",
        "annotated_image"
      ],
      "metadata": {
        "id": "K66PSuHpEKI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Display result with NMS\n",
        "\n",
        "detections = sv.Detections.from_transformers(results[0]).with_nms(threshold=0.1)\n",
        "labels = [\n",
        "    model.config.id2label[class_id]\n",
        "    for class_id\n",
        "    in detections.class_id\n",
        "]\n",
        "\n",
        "annotated_image = image.copy()\n",
        "annotated_image = sv.BoundingBoxAnnotator().annotate(annotated_image, detections)\n",
        "annotated_image = sv.LabelAnnotator().annotate(annotated_image, detections, labels=labels)\n",
        "annotated_image.thumbnail((600, 600))\n",
        "annotated_image"
      ],
      "metadata": {
        "id": "NxgvSRLXvaEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune RT-DETR on custom dataset"
      ],
      "metadata": {
        "id": "epVdXdCYyfa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download dataset from Roboflow Universe\n",
        "\n",
        "ROBOFLOW_API_KEY = userdata.get('ROBOFLOW_API_KEY')\n",
        "rf = Roboflow(api_key=ROBOFLOW_API_KEY)\n",
        "\n",
        "project = rf.workspace(\"roboflow-jvuqo\").project(\"poker-cards-fmjio\")\n",
        "version = project.version(4)\n",
        "dataset = version.download(\"coco\")"
      ],
      "metadata": {
        "id": "r4nOMZK4vafh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train = sv.DetectionDataset.from_coco(\n",
        "    images_directory_path=f\"{dataset.location}/train\",\n",
        "    annotations_path=f\"{dataset.location}/train/_annotations.coco.json\",\n",
        ")\n",
        "ds_valid = sv.DetectionDataset.from_coco(\n",
        "    images_directory_path=f\"{dataset.location}/valid\",\n",
        "    annotations_path=f\"{dataset.location}/valid/_annotations.coco.json\",\n",
        ")\n",
        "ds_test = sv.DetectionDataset.from_coco(\n",
        "    images_directory_path=f\"{dataset.location}/test\",\n",
        "    annotations_path=f\"{dataset.location}/test/_annotations.coco.json\",\n",
        ")\n",
        "\n",
        "print(f\"Number of training images: {len(ds_train)}\")\n",
        "print(f\"Number of validation images: {len(ds_valid)}\")\n",
        "print(f\"Number of test images: {len(ds_test)}\")"
      ],
      "metadata": {
        "id": "dNUoM-C-zH4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Display dataset sample\n",
        "\n",
        "GRID_SIZE = 5\n",
        "\n",
        "def annotate(image, annotations, classes):\n",
        "    labels = [\n",
        "        classes[class_id]\n",
        "        for class_id\n",
        "        in annotations.class_id\n",
        "    ]\n",
        "\n",
        "    bounding_box_annotator = sv.BoundingBoxAnnotator()\n",
        "    label_annotator = sv.LabelAnnotator(text_scale=1, text_thickness=2)\n",
        "\n",
        "    annotated_image = image.copy()\n",
        "    annotated_image = bounding_box_annotator.annotate(annotated_image, annotations)\n",
        "    annotated_image = label_annotator.annotate(annotated_image, annotations, labels=labels)\n",
        "    return annotated_image\n",
        "\n",
        "annotated_images = []\n",
        "for i in range(GRID_SIZE * GRID_SIZE):\n",
        "    _, image, annotations = ds_train[i]\n",
        "    annotated_image = annotate(image, annotations, ds_train.classes)\n",
        "    annotated_images.append(annotated_image)\n",
        "\n",
        "grid = sv.create_tiles(\n",
        "    annotated_images,\n",
        "    grid_size=(GRID_SIZE, GRID_SIZE),\n",
        "    single_tile_size=(400, 400),\n",
        "    tile_padding_color=sv.Color.WHITE,\n",
        "    tile_margin_color=sv.Color.WHITE\n",
        ")\n",
        "sv.plot_image(grid, size=(10, 10))"
      ],
      "metadata": {
        "id": "ykYVyyPI0MdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess the data\n",
        "\n",
        "To finetune a model, you must preprocess the data you plan to use to match precisely the approach used for the pre-trained model. [AutoImageProcessor](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor) takes care of processing image data to create `pixel_values`, `pixel_mask`, and `labels` that a DETR model can train with. The image processor has some attributes that you won't have to worry about:\n",
        "\n",
        "- `image_mean = [0.485, 0.456, 0.406 ]`\n",
        "- `image_std = [0.229, 0.224, 0.225]`\n",
        "\n",
        "These are the mean and standard deviation used to normalize images during the model pre-training. These values are crucial to replicate when doing inference or finetuning a pre-trained image model.\n",
        "\n",
        "Instantiate the image processor from the same checkpoint as the model you want to finetune."
      ],
      "metadata": {
        "id": "r3YGacvb3qDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 480\n",
        "\n",
        "processor = AutoImageProcessor.from_pretrained(\n",
        "    CHECKPOINT,\n",
        "    do_resize=True,\n",
        "    size={\"width\": IMAGE_SIZE, \"height\": IMAGE_SIZE},\n",
        ")"
      ],
      "metadata": {
        "id": "T1rf4rDG0tRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before passing the images to the `processor`, apply two preprocessing transformations to the dataset:\n",
        "\n",
        "- Augmenting images\n",
        "- Reformatting annotations to meet RT-DETR expectations\n",
        "\n",
        "First, to make sure the model does not overfit on the training data, you can apply image augmentation with any data augmentation library. Here we use [Albumentations](https://albumentations.ai/docs/). This library ensures that transformations affect the image and update the bounding boxes accordingly."
      ],
      "metadata": {
        "id": "f9P2kK_86ddM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_augmentation_and_transform = A.Compose(\n",
        "    [\n",
        "        A.Perspective(p=0.1),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.RandomBrightnessContrast(p=0.5),\n",
        "        A.HueSaturationValue(p=0.1),\n",
        "    ],\n",
        "    bbox_params=A.BboxParams(\n",
        "        format=\"pascal_voc\",\n",
        "        label_fields=[\"category\"],\n",
        "        clip=True,\n",
        "        min_area=25\n",
        "    ),\n",
        ")\n",
        "\n",
        "valid_transform = A.Compose(\n",
        "    [A.NoOp()],\n",
        "    bbox_params=A.BboxParams(\n",
        "        format=\"pascal_voc\",\n",
        "        label_fields=[\"category\"],\n",
        "        clip=True,\n",
        "        min_area=1\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "0O8aejla26pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualize some augmented images\n",
        "\n",
        "IMAGE_COUNT = 5\n",
        "\n",
        "for i in range(IMAGE_COUNT):\n",
        "    _, image, annotations = ds_train[i]\n",
        "\n",
        "    output = train_augmentation_and_transform(\n",
        "        image=image,\n",
        "        bboxes=annotations.xyxy,\n",
        "        category=annotations.class_id\n",
        "    )\n",
        "\n",
        "    augmented_image = output[\"image\"]\n",
        "    augmented_annotations = replace(\n",
        "        annotations,\n",
        "        xyxy=np.array(output[\"bboxes\"]),\n",
        "        class_id=np.array(output[\"category\"])\n",
        "    )\n",
        "\n",
        "    annotated_images = [\n",
        "        annotate(image, annotations, ds_train.classes),\n",
        "        annotate(augmented_image, augmented_annotations, ds_train.classes)\n",
        "    ]\n",
        "    grid = sv.create_tiles(\n",
        "        annotated_images,\n",
        "        titles=['original', 'augmented'],\n",
        "        titles_scale=0.5,\n",
        "        single_tile_size=(400, 400),\n",
        "        tile_padding_color=sv.Color.WHITE,\n",
        "        tile_margin_color=sv.Color.WHITE\n",
        "    )\n",
        "    sv.plot_image(grid, size=(6, 6))"
      ],
      "metadata": {
        "id": "pDzj_qqm7fAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `processor` expects the annotations to be in the following format: `{'image_id': int, 'annotations': List[Dict]}`, where each dictionary is a COCO object annotation. Let's add a function to reformat annotations for a single example:"
      ],
      "metadata": {
        "id": "17y_b4RoCPBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PyTorchDetectionDataset(Dataset):\n",
        "    def __init__(self, dataset: sv.DetectionDataset, processor, transform: A.Compose = None):\n",
        "        self.dataset = dataset\n",
        "        self.processor = processor\n",
        "        self.transform = transform\n",
        "\n",
        "    @staticmethod\n",
        "    def annotations_as_coco(image_id, categories, boxes):\n",
        "        annotations = []\n",
        "        for category, bbox in zip(categories, boxes):\n",
        "            x1, y1, x2, y2 = bbox\n",
        "            formatted_annotation = {\n",
        "                \"image_id\": image_id,\n",
        "                \"category_id\": category,\n",
        "                \"bbox\": [x1, y1, x2 - x1, y2 - y1],\n",
        "                \"iscrowd\": 0,\n",
        "                \"area\": (x2 - x1) * (y2 - y1),\n",
        "            }\n",
        "            annotations.append(formatted_annotation)\n",
        "\n",
        "        return {\n",
        "            \"image_id\": image_id,\n",
        "            \"annotations\": annotations,\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        _, image, annotations = self.dataset[idx]\n",
        "\n",
        "        # Convert image to RGB numpy array\n",
        "        image = image[:, :, ::-1]\n",
        "        boxes = annotations.xyxy\n",
        "        categories = annotations.class_id\n",
        "\n",
        "        if self.transform:\n",
        "            transformed = self.transform(\n",
        "                image=image,\n",
        "                bboxes=boxes,\n",
        "                category=categories\n",
        "            )\n",
        "            image = transformed[\"image\"]\n",
        "            boxes = transformed[\"bboxes\"]\n",
        "            categories = transformed[\"category\"]\n",
        "\n",
        "\n",
        "        formatted_annotations = self.annotations_as_coco(\n",
        "            image_id=idx, categories=categories, boxes=boxes)\n",
        "        result = self.processor(\n",
        "            images=image, annotations=formatted_annotations, return_tensors=\"pt\")\n",
        "\n",
        "        # Image processor expands batch dimension, lets squeeze it\n",
        "        result = {k: v[0] for k, v in result.items()}\n",
        "\n",
        "        return result"
      ],
      "metadata": {
        "id": "oMO15Iro-IUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can combine the image and annotation transformations to use on a batch of examples:"
      ],
      "metadata": {
        "id": "kMpvQ4kwEkDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pytorch_dataset_train = PyTorchDetectionDataset(\n",
        "    ds_train, processor, transform=train_augmentation_and_transform)\n",
        "pytorch_dataset_valid = PyTorchDetectionDataset(\n",
        "    ds_valid, processor, transform=valid_transform)\n",
        "pytorch_dataset_test = PyTorchDetectionDataset(\n",
        "    ds_test, processor, transform=valid_transform)\n",
        "\n",
        "pytorch_dataset_train[15]"
      ],
      "metadata": {
        "id": "4RGFftpAEaQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have successfully augmented the images and prepared their annotations. In the final step, create a custom collate_fn to batch images together."
      ],
      "metadata": {
        "id": "h8KHJfSZFh7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    data = {}\n",
        "    data[\"pixel_values\"] = torch.stack([x[\"pixel_values\"] for x in batch])\n",
        "    data[\"labels\"] = [x[\"labels\"] for x in batch]\n",
        "    return data"
      ],
      "metadata": {
        "id": "OgTqutO9FDM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing function to compute mAP"
      ],
      "metadata": {
        "id": "VH_APRS_GP0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {id: label for id, label in enumerate(ds_train.classes)}\n",
        "label2id = {label: id for id, label in enumerate(ds_train.classes)}\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelOutput:\n",
        "    logits: torch.Tensor\n",
        "    pred_boxes: torch.Tensor\n",
        "\n",
        "\n",
        "class MAPEvaluator:\n",
        "\n",
        "    def __init__(self, image_processor, threshold=0.00, id2label=None):\n",
        "        self.image_processor = image_processor\n",
        "        self.threshold = threshold\n",
        "        self.id2label = id2label\n",
        "\n",
        "    def collect_image_sizes(self, targets):\n",
        "        \"\"\"Collect image sizes across the dataset as list of tensors with shape [batch_size, 2].\"\"\"\n",
        "        image_sizes = []\n",
        "        for batch in targets:\n",
        "            batch_image_sizes = torch.tensor(np.array([x[\"size\"] for x in batch]))\n",
        "            image_sizes.append(batch_image_sizes)\n",
        "        return image_sizes\n",
        "\n",
        "    def collect_targets(self, targets, image_sizes):\n",
        "        post_processed_targets = []\n",
        "        for target_batch, image_size_batch in zip(targets, image_sizes):\n",
        "            for target, (height, width) in zip(target_batch, image_size_batch):\n",
        "                boxes = target[\"boxes\"]\n",
        "                boxes = sv.xcycwh_to_xyxy(boxes)\n",
        "                boxes = boxes * np.array([width, height, width, height])\n",
        "                boxes = torch.tensor(boxes)\n",
        "                labels = torch.tensor(target[\"class_labels\"])\n",
        "                post_processed_targets.append({\"boxes\": boxes, \"labels\": labels})\n",
        "        return post_processed_targets\n",
        "\n",
        "    def collect_predictions(self, predictions, image_sizes):\n",
        "        post_processed_predictions = []\n",
        "        for batch, target_sizes in zip(predictions, image_sizes):\n",
        "            batch_logits, batch_boxes = batch[1], batch[2]\n",
        "            output = ModelOutput(logits=torch.tensor(batch_logits), pred_boxes=torch.tensor(batch_boxes))\n",
        "            post_processed_output = self.image_processor.post_process_object_detection(\n",
        "                output, threshold=self.threshold, target_sizes=target_sizes\n",
        "            )\n",
        "            post_processed_predictions.extend(post_processed_output)\n",
        "        return post_processed_predictions\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(self, evaluation_results):\n",
        "\n",
        "        predictions, targets = evaluation_results.predictions, evaluation_results.label_ids\n",
        "\n",
        "        image_sizes = self.collect_image_sizes(targets)\n",
        "        post_processed_targets = self.collect_targets(targets, image_sizes)\n",
        "        post_processed_predictions = self.collect_predictions(predictions, image_sizes)\n",
        "\n",
        "        evaluator = MeanAveragePrecision(box_format=\"xyxy\", class_metrics=True)\n",
        "        evaluator.warn_on_many_detections = False\n",
        "        evaluator.update(post_processed_predictions, post_processed_targets)\n",
        "\n",
        "        metrics = evaluator.compute()\n",
        "\n",
        "        # Replace list of per class metrics with separate metric for each class\n",
        "        classes = metrics.pop(\"classes\")\n",
        "        map_per_class = metrics.pop(\"map_per_class\")\n",
        "        mar_100_per_class = metrics.pop(\"mar_100_per_class\")\n",
        "        for class_id, class_map, class_mar in zip(classes, map_per_class, mar_100_per_class):\n",
        "            class_name = id2label[class_id.item()] if id2label is not None else class_id.item()\n",
        "            metrics[f\"map_{class_name}\"] = class_map\n",
        "            metrics[f\"mar_100_{class_name}\"] = class_mar\n",
        "\n",
        "        metrics = {k: round(v.item(), 4) for k, v in metrics.items()}\n",
        "\n",
        "        return metrics\n",
        "\n",
        "eval_compute_metrics_fn = MAPEvaluator(image_processor=processor, threshold=0.01, id2label=id2label)"
      ],
      "metadata": {
        "id": "9Va2ok5MGZ0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the detection model\n",
        "\n",
        "You have done most of the heavy lifting in the previous sections, so now you are ready to train your model! The images in this dataset are still quite large, even after resizing. This means that finetuning this model will require at least one GPU.\n",
        "\n",
        "Training involves the following steps:\n",
        "\n",
        "- Load the model with [`AutoModelForObjectDetection`](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForObjectDetection) using the same checkpoint as in the preprocessing.\n",
        "- Define your training hyperparameters in [`TrainingArguments`](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments).\n",
        "- Pass the training arguments to [`Trainer`](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) along with the model, dataset, image processor, and data collator.\n",
        "- Call [`train()`](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) to finetune your model.\n",
        "\n",
        "When loading the model from the same checkpoint that you used for the preprocessing, remember to pass the `label2id` and `id2label` maps that you created earlier from the dataset's metadata. Additionally, we specify `ignore_mismatched_sizes=True` to replace the existing classification head with a new one."
      ],
      "metadata": {
        "id": "vYufsMq0He6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForObjectDetection.from_pretrained(\n",
        "    CHECKPOINT,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    anchor_image_size=None,\n",
        "    ignore_mismatched_sizes=True,\n",
        ")"
      ],
      "metadata": {
        "id": "a7kZr8OtGkIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the [`TrainingArguments`](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments) use `output_dir` to specify where to save your model, then configure hyperparameters as you see fit. For `num_train_epochs=10` training will take about 15 minutes in Google Colab T4 GPU, increase the number of epoch to get better results.\n",
        "\n",
        "Important notes:\n",
        "\n",
        "- Do not remove unused columns because this will drop the image column. Without the image column, you can't create `pixel_values`. For this reason, set `remove_unused_columns` to `False`.\n",
        "- Set `eval_do_concat_batches=False` to get proper evaluation results. Images have different number of target boxes, if batches are concatenated we will not be able to determine which boxes belongs to particular image."
      ],
      "metadata": {
        "id": "fSUNE6p8IUN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"{dataset.name.replace(' ', '-')}-finetune\",\n",
        "    num_train_epochs=20,\n",
        "    max_grad_norm=0.1,\n",
        "    learning_rate=5e-5,\n",
        "    warmup_steps=300,\n",
        "    per_device_train_batch_size=16,\n",
        "    dataloader_num_workers=2,\n",
        "    metric_for_best_model=\"eval_map\",\n",
        "    greater_is_better=True,\n",
        "    load_best_model_at_end=True,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    remove_unused_columns=False,\n",
        "    eval_do_concat_batches=False,\n",
        ")"
      ],
      "metadata": {
        "id": "Jb2ScVfxIFnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, bring everything together, and call [`train()`](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train):"
      ],
      "metadata": {
        "id": "WmIV8s4hKiOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=pytorch_dataset_train,\n",
        "    eval_dataset=pytorch_dataset_valid,\n",
        "    tokenizer=processor,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=eval_compute_metrics_fn,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Fh5y-l8LJLIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate"
      ],
      "metadata": {
        "id": "wupI4QR0PQTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Collect predictions\n",
        "\n",
        "targets = []\n",
        "predictions = []\n",
        "\n",
        "for i in range(len(ds_test)):\n",
        "    path, sourece_image, annotations = ds_test[i]\n",
        "\n",
        "    image = Image.open(path)\n",
        "    inputs = processor(image, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    w, h = image.size\n",
        "    results = processor.post_process_object_detection(\n",
        "        outputs, target_sizes=[(h, w)], threshold=0.3)\n",
        "\n",
        "    detections = sv.Detections.from_transformers(results[0])\n",
        "\n",
        "    targets.append(annotations)\n",
        "    predictions.append(detections)"
      ],
      "metadata": {
        "id": "ngPVGIFew9zC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Calculate mAP\n",
        "mean_average_precision = sv.MeanAveragePrecision.from_detections(\n",
        "    predictions=predictions,\n",
        "    targets=targets,\n",
        ")\n",
        "\n",
        "print(f\"map50_95: {mean_average_precision.map50_95:.2f}\")\n",
        "print(f\"map50: {mean_average_precision.map50:.2f}\")\n",
        "print(f\"map75: {mean_average_precision.map75:.2f}\")"
      ],
      "metadata": {
        "id": "WYxQ46GxxyZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Calculate Confusion Matrix\n",
        "confusion_matrix = sv.ConfusionMatrix.from_detections(\n",
        "    predictions=predictions,\n",
        "    targets=targets,\n",
        "    classes=ds_test.classes\n",
        ")\n",
        "\n",
        "_ = confusion_matrix.plot()"
      ],
      "metadata": {
        "id": "bGewqyx9xheE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save fine-tuned model on hard drive"
      ],
      "metadata": {
        "id": "VkS0nmBDP8GR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/rt-detr/\")\n",
        "processor.save_pretrained(\"/content/rt-detr/\")"
      ],
      "metadata": {
        "id": "CAxMHJ1lP9M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference with fine-tuned RT-DETR model"
      ],
      "metadata": {
        "id": "apNLYNASPo4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_COUNT = 5\n",
        "\n",
        "for i in range(IMAGE_COUNT):\n",
        "    path, sourece_image, annotations = ds_test[i]\n",
        "\n",
        "    image = Image.open(path)\n",
        "    inputs = processor(image, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    w, h = image.size\n",
        "    results = processor.post_process_object_detection(\n",
        "        outputs, target_sizes=[(h, w)], threshold=0.3)\n",
        "\n",
        "    detections = sv.Detections.from_transformers(results[0]).with_nms(threshold=0.1)\n",
        "\n",
        "    annotated_images = [\n",
        "        annotate(sourece_image, annotations, ds_train.classes),\n",
        "        annotate(sourece_image, detections, ds_train.classes)\n",
        "    ]\n",
        "    grid = sv.create_tiles(\n",
        "        annotated_images,\n",
        "        titles=['ground truth', 'prediction'],\n",
        "        titles_scale=0.5,\n",
        "        single_tile_size=(400, 400),\n",
        "        tile_padding_color=sv.Color.WHITE,\n",
        "        tile_margin_color=sv.Color.WHITE\n",
        "    )\n",
        "    sv.plot_image(grid, size=(6, 6))"
      ],
      "metadata": {
        "id": "5ReJ7Z1xPaJk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}